{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully!\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Activation\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=(256,256,3), activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(Conv2D(128, (3, 3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))         #(2,2) ?\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024,activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(1,  activation='relu'))\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "print('Model compiled successfully!')\n",
    "model.save(filepath=\"/storage/Manual Model.h5\",\n",
    "               overwrite=True,include_optimizer=True,save_format='h5')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40000 validated image filenames.\n",
      "Found 5000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/storage/data.csv\")\n",
    "df['filename']=\"clips-\"+df[\"id\"].astype(str)+\".png\"\n",
    "\n",
    "df_validate = pd.read_csv(\"/storage/validate.csv\", na_values=['NA', '?'])\n",
    "df_validate['filename']=\"clips-\"+df_validate[\"id\"].astype(str)+\".png\"\n",
    "\n",
    "\n",
    "df_train = df[:40000] #45000 images in total\n",
    "df_test = df[40000:45000]\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGES_DIR = \"/storage/clips\"\n",
    "\n",
    "training_datagen = ImageDataGenerator(rescale = 1./255., horizontal_flip=True, \n",
    "                                      vertical_flip=True, fill_mode='nearest')\n",
    "train_generator = training_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=IMAGES_DIR,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"clip_count\",\n",
    "        target_size=(256, 256),\n",
    "        batch_size=64,\n",
    "        class_mode='other')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=df_test,\n",
    "        directory=IMAGES_DIR,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"clip_count\",\n",
    "        target_size=(256, 256),\n",
    "        class_mode='other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created callback: Monitor\n",
      "Successfully created callback: Checkpoint\n",
      "Epoch 112/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3808\n",
      "Epoch 00112: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 661s 1s/step - loss: 0.3807 - val_loss: 0.5533\n",
      "Epoch 113/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3651\n",
      "Epoch 00113: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 659s 1s/step - loss: 0.3652 - val_loss: 0.8318\n",
      "Epoch 114/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3576\n",
      "Epoch 00114: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 662s 1s/step - loss: 0.3578 - val_loss: 0.5594\n",
      "Epoch 115/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3829\n",
      "Epoch 00115: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 661s 1s/step - loss: 0.3828 - val_loss: 0.6808\n",
      "Epoch 116/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3668\n",
      "Epoch 00116: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 661s 1s/step - loss: 0.3668 - val_loss: 0.5502\n",
      "Epoch 117/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3612\n",
      "Epoch 00117: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 661s 1s/step - loss: 0.3624 - val_loss: 0.5728\n",
      "Epoch 118/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.4041\n",
      "Epoch 00118: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 661s 1s/step - loss: 0.4043 - val_loss: 0.5469\n",
      "Epoch 119/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3750\n",
      "Epoch 00119: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 624s 998ms/step - loss: 0.3749 - val_loss: 0.6379\n",
      "Epoch 120/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3439\n",
      "Epoch 00120: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 656s 1s/step - loss: 0.3439 - val_loss: 0.7164\n",
      "Epoch 121/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3493\n",
      "Epoch 00121: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 659s 1s/step - loss: 0.3493 - val_loss: 0.6873\n",
      "Epoch 122/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3564\n",
      "Epoch 00122: saving model to /storage/Manual Model.h5\n",
      "625/625 [==============================] - 660s 1s/step - loss: 0.3563 - val_loss: 0.6429\n",
      "Epoch 123/200\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.3846\n",
      "Epoch 00123: saving model to /storage/Manual Model.h5\n",
      "Restoring model weights from the end of the best epoch.\n",
      "625/625 [==============================] - 660s 1s/step - loss: 0.3846 - val_loss: 0.6174\n",
      "Epoch 00123: early stopping\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "if(\"model\" not in globals().keys()):\n",
    "    model=load_model(\"/storage/Manual Model.h5\",compile=False)\n",
    "    model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "    print(\"Successfully loaded h5 model!\")\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',\n",
    "        restore_best_weights=True)\n",
    "print(\"Successfully created callback: Monitor\")\n",
    "checkpoint = ModelCheckpoint(\"/storage/Manual Model.h5\", \n",
    "                             monitor='val_loss', verbose=1)\n",
    "print(\"Successfully created callback: Checkpoint\")\n",
    "history = model.fit_generator(train_generator, validation_data=test_generator, steps_per_epoch=625,\n",
    "                    callbacks=[checkpoint,monitor], epochs=200, verbose = 1,\n",
    "                    initial_epoch=111) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model=load_model(\"/storage/2020-04-06(0.61).h5\",compile=False)\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\n",
    "print(\"Successfully loaded h5 model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_validate = pd.read_csv(\"/storage/validate.csv\", na_values=['NA', '?'])\n",
    "df_validate['filename']=\"clips-\"+df_validate[\"id\"].astype(str)+\".png\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGES_DIR = \"/storage/clips\"\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=df_validate,\n",
    "        directory=IMAGES_DIR,\n",
    "        x_col=\"filename\",\n",
    "        y_col=\"id\",\n",
    "        target_size=(256, 256),\n",
    "        class_mode='other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [04:10<00:00, 19.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>2.054140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>47.178310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>63.330708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>3.544874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25005</th>\n",
       "      <td>44.842930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>31.811995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>55.457638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>61.454952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>4.945584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>31.279673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           clips\n",
       "25001   2.054140\n",
       "25002  47.178310\n",
       "25003  63.330708\n",
       "25004   3.544874\n",
       "25005  44.842930\n",
       "...          ...\n",
       "29996  31.811995\n",
       "29997  55.457638\n",
       "29998  61.454952\n",
       "29999   4.945584\n",
       "30000  31.279673\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(5000)):\n",
    "    arr=np.expand_dims(np.array(Image.open(f\"/storage/clips/clips-{25001+i}.png\"))/255, 0)\n",
    "    df.loc[i+25001,\"clips\"]=model.predict(arr)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/notebooks/submit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5787314589089702"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"/notebooks/test_prediction.csv\")\n",
    "df.columns=[\"id\",\"clip_count\"]\n",
    "df[\"clip_count\"]=df[\"clip_count\"].apply(lambda x: 75 if x>75 else x)\n",
    "df[\"decimal\"]=df[\"clip_count\"]-df[\"clip_count\"].astype(\"int\")\n",
    "df[\"integer\"]=df[\"clip_count\"].astype(\"int\")\n",
    "df[\"True count\"]=pd.read_csv(\"/storage/data.csv\")[\"clip_count\"][-5000:].values\n",
    "df[\"error\"]=df[\"True count\"]-df[\"clip_count\"]\n",
    "#display(df)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(df[\"True count\"],df[\"clip_count\"])\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "display(mean_squared_error(df[\"clip_count\"],df[\"True count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
